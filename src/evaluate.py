model = BertForMaskedLM.from_pretrained("bert-base-uncased")
        self.tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    def calculate_perplexity(self, text):
        """
        Use BertForMaskedLM to compute the perplexity by masking the symbols one by one
        """
        # Tokenized text
        encodings = self.tokenizer(text, return_tensors="pt")
        input_ids = encodings["input_ids"].squeeze()

        # Initialize the perplexity calculation variables
        total_loss = 0
        total_tokens = len(input_ids)

        # Calculate the loss at each position
        for i in range(1, total_tokens-1):
            # Creating an Occlusion Input
            masked_input_ids = input_ids.clone()
            original_token = masked_input_ids[i].item()
            masked_input_ids[i] = self.tokenizer.mask_token_id

            # Get model predictions
            with torch.no_grad():
                outputs = self.bert_model(masked_input_ids.unsqueeze(0))
                logits = outputs.logits

            # Calculate the loss at this location
            target = torch.tensor([original_token])
            predicted_logits = logits[0, i]
            loss = torch.nn.functional.cross_entropy(predicted_logits.unsqueeze(0), target)
            total_loss += loss.item()

        # Calculating perplexity
        avg_loss = total_loss / (total_tokens - 2)
        perplexity = torch.exp(torch.tensor(avg_loss)).item()
        return perplexity

    def calculate_readability(self, text):
        """
        Calculating the Flesch-Kincaid readability score
        """
        return textstat.flesch_reading_ease(text)

    def evaluate_text(self, generated_text):
        """
        Evaluating the generated text
        """
        perplexity = self.calculate_perplexity(generated_text)
        readability = self.calculate_readability(generated_text)

        return {
            "Perplexity (Lower is Better)": round(perplexity, 4),
            "Readability (Flesch Score, Higher is Easier)": round(readability, 2)
        }

def evaluate_model_generation(model_path, prompt, max_length=256):
    """Evaluate the quality of text generated by the model"""
    
    # If a model path is provided, the model is loaded
    if model_path and os.path.exists(model_path):
        print(f"Loading model from {model_path}")
        gemma_lm = keras.models.load_model(model_path)
    else:
        print("Using default Gemma model...")
        gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma2_2b_en")
    
    # Setting up the Sampler
    sampler = keras_nlp.samplers.TopKSampler(k=5, seed=42)
    gemma_lm.compile(sampler=sampler)
    
    # Generating text
    template = "Instruction:\n{instruction}\n\nResponse:\n{response}"
    full_prompt = template.format(instruction=prompt, response="")
    
    print(f"\nGenerating text for prompt: '{prompt}'")
    generated_text = gemma_lm.generate(full_prompt, max_length=max_length)
    
    # Get the generated response part
    response_text = generated_text.split("Response:\n")[1].strip()
    
    # Evaluating the generated text
    evaluator = TextEvaluator()
    results = evaluator.evaluate_text(response_text)
    
    print("\nGenerated text:")
    print("-" * 50)
    print(response_text)
    print("-" * 50)
    print("\nEvaluation Results:")
    for metric, value in results.items():
        print(f"{metric}: {value}")
        
    return results

def main():
    parser = argparse.ArgumentParser(description="Evaluate the quality of text generated by the Gemma model")
    parser.add_argument("--model-path", type=str, default="", help="Path to the trained model")
    parser.add_argument("--prompt", type=str, default="What should I do on a trip to Europe?",
                        help="Prompt for generated text")
    parser.add_argument("--max-length", type=int, default=256, help="Maximum length of generated text")
    
    args = parser.parse_args()
    
    evaluate_model_generation(args.model_path, args.prompt, args.max_length)

if __name__ == "__main__":
    main()